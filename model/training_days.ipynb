{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ec34b8-0bb3-487e-9d11-98ffa61a0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c459028-3062-46c6-8dbc-f2baa9467137",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# fix random number generation aka regenerate the same random numbers every time (such as weight and bias initialization )\n",
    "def set_random_seed(seed=7, deterministic=True):\n",
    "    \"\"\"Set random seed, for python, numpy, pytorch\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed to be used.\n",
    "        deterministic (bool): Whether to set the deterministic option for\n",
    "            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`\n",
    "            to True and `torch.backends.cudnn.benchmark` to False.\n",
    "            Default: True.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False \n",
    "seed=7        \n",
    "set_random_seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f19486a-ed28-4936-86d3-2fd667902629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_name</th>\n",
       "      <th>month</th>\n",
       "      <th>leap_year_condition</th>\n",
       "      <th>decade</th>\n",
       "      <th>output</th>\n",
       "      <th>output_year_digit</th>\n",
       "      <th>output_year</th>\n",
       "      <th>valid_years_list</th>\n",
       "      <th>valid_day_list</th>\n",
       "      <th>decade4</th>\n",
       "      <th>decade100</th>\n",
       "      <th>decade400</th>\n",
       "      <th>valid_group_days_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1800</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1801</td>\n",
       "      <td>1</td>\n",
       "      <td>1801</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1802</td>\n",
       "      <td>2</td>\n",
       "      <td>1802</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1803</td>\n",
       "      <td>3</td>\n",
       "      <td>1803</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1804</td>\n",
       "      <td>4</td>\n",
       "      <td>1804</td>\n",
       "      <td>[4, 8, 4, 4, 4, 4, 4, 4]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1805</td>\n",
       "      <td>5</td>\n",
       "      <td>1805</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>1-1-1806</td>\n",
       "      <td>6</td>\n",
       "      <td>1806</td>\n",
       "      <td>[0, 1, 2, 3, 5, 6, 7, 9]</td>\n",
       "      <td>[1, 8, 15, 22, 29]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weekday_name  month  leap_year_condition  decade    output  \\\n",
       "0             2      1                    0     180  1-1-1800   \n",
       "1             3      1                    0     180  1-1-1801   \n",
       "2             4      1                    0     180  1-1-1802   \n",
       "3             5      1                    0     180  1-1-1803   \n",
       "4             6      1                    1     180  1-1-1804   \n",
       "5             1      1                    0     180  1-1-1805   \n",
       "6             2      1                    0     180  1-1-1806   \n",
       "\n",
       "   output_year_digit  output_year          valid_years_list  \\\n",
       "0                  0         1800  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "1                  1         1801  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "2                  2         1802  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "3                  3         1803  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "4                  4         1804  [4, 8, 4, 4, 4, 4, 4, 4]   \n",
       "5                  5         1805  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "6                  6         1806  [0, 1, 2, 3, 5, 6, 7, 9]   \n",
       "\n",
       "       valid_day_list  decade4  decade100  decade400  valid_group_days_index  \n",
       "0  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "1  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "2  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "3  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "4  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "5  [1, 8, 15, 22, 29]        0          0          1                       7  \n",
       "6  [1, 8, 15, 22, 29]        0          0          1                       7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4051fc1-2734-4548-9329-1d083a98f389",
   "metadata": {},
   "source": [
    "- given the columns features we can't by any means diffrentiate between the day 1 or day 8 or 15 or 22 or 28 because they may share the same exact feature columns values and they all still correct, and the model see them all as the same thing, so i make lists for the identical days from the feature columns point of view, and instead of training the model to select certain day, i trained the model to select index of days-list which i named \"groups\", as example if the output of the model is 0 then it equivalent to selecting any day from these days [1, 8, 15, 22], if the output is 1 so it's equivalent to selecting any number from [2, 9, 16, 23] and so on(see groups_dict dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00bb579d-7822-4ee1-acca-a13fe49ac40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [1, 8, 15, 22],\n",
       " 1: [2, 9, 16, 23],\n",
       " 2: [3, 10, 17, 24],\n",
       " 3: [4, 11, 18, 25],\n",
       " 4: [5, 12, 19, 26],\n",
       " 5: [6, 13, 20, 27],\n",
       " 6: [7, 14, 21, 28],\n",
       " 7: [1, 8, 15, 22, 29],\n",
       " 8: [2, 9, 16, 23, 30],\n",
       " 9: [3, 10, 17, 24, 31]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of all possibole (10) output lists, the output must be one of theses lists\n",
    "groups = [[1, 8, 15, 22], \n",
    "         [2, 9, 16, 23], \n",
    "         [3, 10, 17, 24], \n",
    "         [4, 11, 18, 25], \n",
    "         [5, 12, 19, 26], \n",
    "         [6, 13, 20, 27], \n",
    "         [7, 14, 21, 28],\n",
    "         [1, 8, 15, 22, 29], \n",
    "         [2, 9, 16, 23, 30], \n",
    "         [3, 10, 17, 24, 31]]\n",
    "groups_dict = dict(enumerate(groups))\n",
    "groups_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09d7594-25e6-44c6-a479-8eda01194a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_day = df[[\"weekday_name\", \"month\", \"output_year\", \"leap_year_condition\", \"valid_group_days_index\"]]\n",
    "y_day = x_day.pop(\"valid_group_days_index\") # from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5166d6-4cd8-4e9d-81f7-946c9d4820de",
   "metadata": {},
   "source": [
    "**B- training for getting the day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1e5c94-b7b7-4809-8cd5-b9f7e12f8556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    22540\n",
      "8    22055\n",
      "3    19248\n",
      "4    19248\n",
      "5    19248\n",
      "6    19248\n",
      "9    14035\n",
      "2     8020\n",
      "1     1604\n",
      "0     1216\n",
      "Name: valid_group_days_index, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(df.valid_group_days_index.value_counts())\n",
    "plt.bar(df.valid_group_days_index.value_counts().index, df.valid_group_days_index.value_counts())\n",
    "plt.xticks(range(10))\n",
    "plt.grid(axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04053cb-4c77-4f69-95cc-16043a2fafb3",
   "metadata": {},
   "source": [
    "**the output column is imbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb837f1-a80b-4e7a-bca5-57c92b4f1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "xtrain_day, xtest_day, ytrain_day, ytest_day = train_test_split(x_day, y_day, \n",
    "                                                                test_size=0.19, \n",
    "                                                                shuffle=True, \n",
    "                                                                random_state=seed, \n",
    "                                                                stratify=df.valid_group_days_index)\n",
    "\n",
    "# make oversampling to fix the imbalanced classes\n",
    "ros = RandomOverSampler(random_state=7)\n",
    "x_day_train_sampld, y_day_train_sampld = ros.fit_resample(xtrain_day, ytrain_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b2b2c3-33d8-4e16-b680-124fea70d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectedData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.data = torch.tensor(x.values.astype(np.float32))\n",
    "        self.label = torch.tensor(y.values)\n",
    "        self.n_smpl = x.shape[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_smpl    \n",
    "    \n",
    "train_set_day = CollectedData(x_day_train_sampld, y_day_train_sampld)\n",
    "test_set_day = CollectedData(xtest_day, ytest_day)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a6a1a0d-2e8d-4a7a-b0a3-b1f7cac61d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## random_split() isn't suitable here, so that i used train_test_split function\n",
    "# # train_set, test_set = torch.utils.data.random_split(dataset, [len(dataset)-test_len, int(len(dataset)*0.2)])\n",
    "\n",
    "# train_labels_day = torch.tensor(ytrain_day.values.astype(np.float32)) \n",
    "# test_labels_day = torch.tensor(ytest_day.values.astype(np.float32)) \n",
    "# train_input_day = torch.tensor(xtrain_day.values.astype(np.float32)) \n",
    "# test_input_day = torch.tensor(xtest_day.values.astype(np.float32)) \n",
    "\n",
    "# train_set_day = TensorDataset(train_input_day, train_labels_day)\n",
    "# test_set_day = TensorDataset(test_input_day, test_labels_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4471f16-2365-482e-9381-ec6f16ab3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader_day = DataLoader(dataset=train_set_day, shuffle=True, batch_size=batch_size)\n",
    "test_loader_day = DataLoader(dataset=test_set_day, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0569ef04-1b22-4be1-87b5-2fd7bc29e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self, in_features=4, out_features=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 168)\n",
    "        self.fc2 = nn.Linear(168, 401)\n",
    "        # self.fc3 = nn.Linear(401, 24)\n",
    "        # self.fc4 = nn.Linear(20, 20)\n",
    "        # self.fc5 = nn.Linear(20, 20)\n",
    "        self.fc6 = nn.Linear(401, out_features) \n",
    "        self.initialize_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, inpt):\n",
    "        out = F.leaky_relu(self.fc1(inpt))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        # out = F.leaky_relu(self.fc3(out))\n",
    "        # out = F.leaky_relu(self.fc4(out))\n",
    "        # out = F.leaky_relu(self.fc5(out))\n",
    "        out = F.softmax((self.fc6(out)), dim=1)\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "243ae509-c237-4b62-ba60-c1b57ef3bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculating accuracy\n",
    "@torch.no_grad()\n",
    "def calculate_accuracy(model, data_loader=train_loader_day):\n",
    "    model.eval()\n",
    "    \n",
    "    num_correct, num_samples = 0, 0\n",
    "\n",
    "    for data, labels in data_loader:\n",
    "        \n",
    "        # transfering data to cuda\n",
    "        data = data.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "                \n",
    "        preds = model(data)\n",
    "        num_correct += sum(list(preds.argmax(dim=1)==labels))\n",
    "        num_samples += len(labels)\n",
    "    accuracy = num_correct/num_samples\n",
    "    model.train()\n",
    "    return accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7acd5273-75d7-4238-9144-da877ba3dc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "network(\n",
       "  (fc1): Linear(in_features=4, out_features=168, bias=True)\n",
       "  (fc2): Linear(in_features=168, out_features=401, bias=True)\n",
       "  (fc6): Linear(in_features=401, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing the model\n",
    "model = network(in_features=4, out_features=10).to(device)\n",
    "lr = 0.001\n",
    "# loss and optimizer initializing\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=35, verbose=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261be555-e25f-4e65-83b4-5bc540157bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 33\n",
    "def train_model(num_epochs, data_loader=train_loader_day):\n",
    "    num_batches = len(data_loader)\n",
    "    print(f\"starting learning rate = {lr} \\n number of epochs = {num_epochs} \\n number of batches = {num_batches} \\n\")\n",
    "    # starting training loop epochs\n",
    "    result_train_acc, result_test_acc = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        progress = tqdm(enumerate(data_loader), total=num_batches, leave=True)\n",
    "        for batch_idx, (data, labels) in progress: \n",
    "\n",
    "            # convert data to device\n",
    "            data = data.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # getting prediction and loss\n",
    "            preds = model(data)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "            progress.set_description(f\"epoch [{1+epoch}/{num_epochs}], loss={loss.item():0.4f}\")\n",
    "            progress.set_postfix()\n",
    "\n",
    "        train_acc = calculate_accuracy(model, data_loader=train_loader_day)\n",
    "        test_acc = calculate_accuracy(model, data_loader=test_loader_day)\n",
    "        schedular.step(test_acc)\n",
    "        print(f\"after {1+epoch} epoch, train_acc = {(train_acc*100):.2f}%, test_acc = {(test_acc*100):.2f}%, time_elapsed = {((time.time()-start_time)/60):.1f} minuts\")\n",
    "        result_train_acc += [train_acc]\n",
    "        result_test_acc += [test_acc]\n",
    "    return result_train_acc, result_test_acc\n",
    "result_train_acc, result_test_acc = train_model(num_epochs, data_loader=train_loader_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291886a6-0c26-4ac8-819e-554b193fd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(num_epochs)), result_train_acc)\n",
    "plt.title(\"train accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e311c1-4e2c-49f3-b484-7c28e6f2aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(num_epochs)), result_test_acc)\n",
    "plt.title(\"test accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf7e710-d5de-4a5a-913a-af6d8f807df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huzyfa\\AppData\\Local\\Temp/ipykernel_13216/3722298390.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax((self.fc6(out)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday_name</th>\n",
       "      <th>month</th>\n",
       "      <th>output_year</th>\n",
       "      <th>leap_year_condition</th>\n",
       "      <th>predicted_days_group_index</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19729</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1880</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18483</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1837</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103614</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1956</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1819</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97054</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1812</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1850</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2092</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113721</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2038</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111568</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27828 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        weekday_name  month  output_year  leap_year_condition  \\\n",
       "19729              3      2         1880                    1   \n",
       "18483              5     11         1837                    0   \n",
       "103614             6      7         1956                    1   \n",
       "8841               1     11         1819                    0   \n",
       "97054              5      3         1812                    1   \n",
       "...              ...    ...          ...                  ...   \n",
       "22506              3      9         1850                    0   \n",
       "10317              6      2         2092                    1   \n",
       "5001               0      1         1989                    0   \n",
       "113721             1      8         2038                    0   \n",
       "111568             0      3         1890                    0   \n",
       "\n",
       "        predicted_days_group_index  out  \n",
       "19729                            4    4  \n",
       "18483                            4    3  \n",
       "103614                           4    7  \n",
       "8841                             4    8  \n",
       "97054                            4    6  \n",
       "...                            ...  ...  \n",
       "22506                            4    4  \n",
       "10317                            4    2  \n",
       "5001                             4    8  \n",
       "113721                           4    9  \n",
       "111568                           4    9  \n",
       "\n",
       "[27828 rows x 6 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_test = xtest_day\n",
    "dt = d_test.copy(deep=True)\n",
    "d_test = torch.tensor(d_test.values.astype(np.float32))\n",
    "preds = model(d_test.to(device=device)).argmax(dim=1).cpu()\n",
    "dt[\"predicted_days_group_index\"] = preds\n",
    "dt[\"out\"] = ytest_day\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61a42130-848a-4aad-bbc3-eb461c4e2183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[\"predicted_days_group_index\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b77e045d-a7b7-47bf-b1e0-8936128ec27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_tensor = torch.tensor(d_test[[\"weekday_name\", \"month\", \"output_year\", \"leap_year_condition\"]].values.astype(np.float32))\n",
    "preds = model(days_tensor.to(device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66548e8b-6d86-41ea-9324-c68afb7d58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../data/day_model_saved\")\n",
    "model = torch.load(\"../data/day_model_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993839c3-cabd-4ba6-9142-9a8e374e471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.to_csv(\"../data/day_predections.csv\", header=True, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
